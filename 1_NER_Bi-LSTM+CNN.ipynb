{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d91450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load packages\"\"\"\n",
    "\n",
    "# preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# modeling\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD, Nadam\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a7ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"Preprocessing\"\"\"\n",
    "    def readfile(filename):\n",
    "        \"\"\"\n",
    "        read file\n",
    "        return format : list of list of lists with IOB annotation\n",
    "        [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "        \"\"\"\n",
    "        f = open(filename)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = line.split(' ')\n",
    "            sentence.append([splits[0],splits[-1]])\n",
    "\n",
    "        if len(sentence) >0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        return sentences\n",
    "\n",
    "    def addCharInformation(Sentences):\n",
    "        \"\"\"\n",
    "        break each word into characters, for later to learn character-level information\n",
    "        \"\"\"\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1]]\n",
    "        return Sentences\n",
    "\n",
    "    def getCasing(word, caseLookup):   \n",
    "        casing = 'other'\n",
    "\n",
    "        numDigits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                numDigits += 1\n",
    "\n",
    "        digitFraction = numDigits / float(len(word))\n",
    "\n",
    "        if word.isdigit(): #Is a digit\n",
    "            casing = 'numeric'\n",
    "        elif digitFraction > 0.5:\n",
    "            casing = 'mainly_numeric'\n",
    "        elif word.islower(): #All lower case\n",
    "            casing = 'allLower'\n",
    "        elif word.isupper(): #All upper case\n",
    "            casing = 'allUpper'\n",
    "        elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "            casing = 'initialUpper'\n",
    "        elif numDigits > 0:\n",
    "            casing = 'contains_digit'\n",
    "\n",
    "        return caseLookup[casing]\n",
    "\n",
    "    def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "        unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "        paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        wordCount = 0\n",
    "        unknownWordCount = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            wordIndices = []    \n",
    "            caseIndices = []\n",
    "            charIndices = []\n",
    "            labelIndices = []\n",
    "\n",
    "            for word,char,label in sentence:  \n",
    "                wordCount += 1\n",
    "                if word in word2Idx:\n",
    "                    wordIdx = word2Idx[word]\n",
    "                elif word.lower() in word2Idx:\n",
    "                    wordIdx = word2Idx[word.lower()]                 \n",
    "                else:\n",
    "                    wordIdx = unknownIdx\n",
    "                    unknownWordCount += 1\n",
    "                charIdx = []\n",
    "                for x in char:\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                #Get the label and map to int            \n",
    "                wordIndices.append(wordIdx)\n",
    "                caseIndices.append(getCasing(word, case2Idx))\n",
    "                charIndices.append(charIdx)\n",
    "                labelIndices.append(label2Idx[label])\n",
    "\n",
    "            dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def padding(Sentences):\n",
    "        maxlen = 52\n",
    "        for sentence in Sentences:\n",
    "            char = sentence[2]\n",
    "            for x in char:\n",
    "                maxlen = max(maxlen,len(x))\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "        return Sentences\n",
    "\n",
    "    def tag_dataset(self, dataset, model):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            correctLabels.append(labels)\n",
    "            predLabels.append(pred)\n",
    "        return predLabels, correctLabels\n",
    "\n",
    "    \"\"\"Model training\"\"\"\n",
    "    def createBatches(data):\n",
    "        l = []\n",
    "        for i in data:\n",
    "            l.append(len(i[0]))\n",
    "        l = set(l)\n",
    "        batches = []\n",
    "        batch_len = []\n",
    "        z = 0\n",
    "        for i in l:\n",
    "            for batch in data:\n",
    "                if len(batch[0]) == i:\n",
    "                    batches.append(batch)\n",
    "                    z += 1\n",
    "            batch_len.append(z)\n",
    "        return batches,batch_len\n",
    "\n",
    "    def iterate_minibatches(dataset,batch_len): \n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            caseing = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,c,ch,l = dt\n",
    "                l = np.expand_dims(l,-1)\n",
    "                tokens.append(t)\n",
    "                caseing.append(c)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "            yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n",
    "\n",
    "    \"\"\"Evaluation\"\"\"\n",
    "    def compute_precision(guessed_sentences, correct_sentences):\n",
    "        assert(len(guessed_sentences) == len(correct_sentences))\n",
    "        correctCount = 0\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for sentenceIdx in range(len(guessed_sentences)):\n",
    "            guessed = guessed_sentences[sentenceIdx]\n",
    "            correct = correct_sentences[sentenceIdx]\n",
    "            assert(len(guessed) == len(correct))\n",
    "            idx = 0\n",
    "            while idx < len(guessed):\n",
    "                if guessed[idx][0] == 'B': #A new chunk starts\n",
    "                    count += 1\n",
    "\n",
    "                    if guessed[idx] == correct[idx]:\n",
    "                        idx += 1\n",
    "                        correctlyFound = True\n",
    "\n",
    "                        while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                            if guessed[idx] != correct[idx]:\n",
    "                                correctlyFound = False\n",
    "\n",
    "                            idx += 1\n",
    "\n",
    "                        if idx < len(guessed):\n",
    "                            if correct[idx][0] == 'I': #The chunk in correct was longer\n",
    "                                correctlyFound = False\n",
    "\n",
    "\n",
    "                        if correctlyFound:\n",
    "                            correctCount += 1\n",
    "                    else:\n",
    "                        idx += 1\n",
    "                else:  \n",
    "                    idx += 1\n",
    "\n",
    "        precision = 0\n",
    "        if count > 0:    \n",
    "            precision = float(correctCount) / count\n",
    "\n",
    "        return precision\n",
    "\n",
    "    def compute_f1(predictions, correct, idx2Label): \n",
    "        label_pred = []    \n",
    "        for sentence in predictions:\n",
    "            label_pred.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "        label_correct = []    \n",
    "        for sentence in correct:\n",
    "            label_correct.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "\n",
    "        #print label_pred\n",
    "        #print label_correct\n",
    "\n",
    "        prec = compute_precision(label_pred, label_correct)\n",
    "        rec = compute_precision(label_correct, label_pred)\n",
    "\n",
    "        f1 = 0\n",
    "        if (rec+prec) > 0:\n",
    "            f1 = 2.0 * prec * rec / (prec + rec);\n",
    "\n",
    "        return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "222c4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initiatiate BiLSTM_CNN class\"\"\"\n",
    "\n",
    "class BiLSTM_CNN(object):\n",
    "    \n",
    "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
    "        \n",
    "        self.epochs = EPOCHS\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
    "        self.lstm_state_size = LSTM_STATE_SIZE\n",
    "        self.conv_size = CONV_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.optimizer = OPTIMIZER\n",
    "        \n",
    "    def readfile(filename):\n",
    "        \"\"\"\n",
    "        read file\n",
    "        return format : list of list of lists with IOB annotation\n",
    "        [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "        \"\"\"\n",
    "        f = open(filename)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = line.split(' ')\n",
    "            sentence.append([splits[0],splits[-1]])\n",
    "\n",
    "        if len(sentence) >0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        return sentences\n",
    "    \n",
    "    def loadData(self):\n",
    "        \"\"\"\n",
    "        Load data and add character information\n",
    "        \"\"\"\n",
    "        self.trainSentences = readfile(\"train.txt\")\n",
    "        self.devSentences = readfile(\"valid.txt\")\n",
    "        self.testSentences = readfile(\"test.txt\")\n",
    "    \n",
    "    def addCharInformatioin(Sentences):\n",
    "        \"\"\"\n",
    "        break each word into characters, for later to learn character-level information\n",
    "        \"\"\"\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            for j,data in enumerate(sentence):\n",
    "                chars = [c for c in data[0]]\n",
    "                Sentences[i][j] = [data[0],chars,data[1]]\n",
    "        return Sentences\n",
    "    \n",
    "    def addCharInfo(self):\n",
    "        self.trainSentences = addCharInformation(self.trainSentences)\n",
    "        self.devSentences = addCharInformation(self.devSentences)\n",
    "        self.testSentences = addCharInformation(self.testSentences)\n",
    "        \n",
    "    def getCasing(word, caseLookup):   \n",
    "        casing = 'other'\n",
    "\n",
    "        numDigits = 0\n",
    "        for char in word:\n",
    "            if char.isdigit():\n",
    "                numDigits += 1\n",
    "\n",
    "        digitFraction = numDigits / float(len(word))\n",
    "\n",
    "        if word.isdigit(): #Is a digit\n",
    "            casing = 'numeric'\n",
    "        elif digitFraction > 0.5:\n",
    "            casing = 'mainly_numeric'\n",
    "        elif word.islower(): #All lower case\n",
    "            casing = 'allLower'\n",
    "        elif word.isupper(): #All upper case\n",
    "            casing = 'allUpper'\n",
    "        elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "            casing = 'initialUpper'\n",
    "        elif numDigits > 0:\n",
    "            casing = 'contains_digit'\n",
    "\n",
    "        return caseLookup[casing]\n",
    "    \n",
    "    def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "        unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "        paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        wordCount = 0\n",
    "        unknownWordCount = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            wordIndices = []    \n",
    "            caseIndices = []\n",
    "            charIndices = []\n",
    "            labelIndices = []\n",
    "\n",
    "            for word,char,label in sentence:  \n",
    "                wordCount += 1\n",
    "                if word in word2Idx:\n",
    "                    wordIdx = word2Idx[word]\n",
    "                elif word.lower() in word2Idx:\n",
    "                    wordIdx = word2Idx[word.lower()]                 \n",
    "                else:\n",
    "                    wordIdx = unknownIdx\n",
    "                    unknownWordCount += 1\n",
    "                charIdx = []\n",
    "                for x in char:\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                #Get the label and map to int            \n",
    "                wordIndices.append(wordIdx)\n",
    "                caseIndices.append(getCasing(word, case2Idx))\n",
    "                charIndices.append(charIdx)\n",
    "                labelIndices.append(label2Idx[label])\n",
    "\n",
    "            dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def padding(Sentences):\n",
    "        maxlen = 52\n",
    "        for sentence in Sentences:\n",
    "            char = sentence[2]\n",
    "            for x in char:\n",
    "                maxlen = max(maxlen,len(x))\n",
    "        for i,sentence in enumerate(Sentences):\n",
    "            Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "        return Sentences\n",
    "        \n",
    "    def embed(self):\n",
    "        \"\"\"\n",
    "        Create word- and character-level embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        labelSet = set()\n",
    "        words = {}\n",
    "\n",
    "        # unique words and labels in data  \n",
    "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
    "            for sentence in dataset:\n",
    "                for token, char, label in sentence:\n",
    "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
    "                    labelSet.add(label)\n",
    "                    words[token.lower()] = True\n",
    "\n",
    "        # mapping for labels\n",
    "        self.label2Idx = {}\n",
    "        for label in labelSet:\n",
    "            self.label2Idx[label] = len(self.label2Idx)\n",
    "\n",
    "        # mapping for token cases\n",
    "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
    "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "        # read GLoVE word embeddings\n",
    "        word2Idx = {}\n",
    "        self.wordEmbeddings = []\n",
    "\n",
    "        fEmbeddings = open(\"glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "        \n",
    "        # loop through each word in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(word2Idx) == 0:  # add padding+unknown\n",
    "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "\n",
    "        # dictionary of all possible characters\n",
    "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "\n",
    "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
    "        self.train_set = padding(createMatrices(self.trainSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "        self.dev_set = padding(createMatrices(self.devSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "        self.test_set = padding(createMatrices(self.testSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "\n",
    "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n",
    "        \n",
    "    def createBatches(self):\n",
    "        \"\"\"Create batches\"\"\"\n",
    "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
    "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
    "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
    "    \n",
    "    def iterate_minibatches(dataset,batch_len): \n",
    "        start = 0\n",
    "        for i in batch_len:\n",
    "            tokens = []\n",
    "            caseing = []\n",
    "            char = []\n",
    "            labels = []\n",
    "            data = dataset[start:i]\n",
    "            start = i\n",
    "            for dt in data:\n",
    "                t,c,ch,l = dt\n",
    "                l = np.expand_dims(l,-1)\n",
    "                tokens.append(t)\n",
    "                caseing.append(c)\n",
    "                char.append(ch)\n",
    "                labels.append(l)\n",
    "            yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n",
    "    \n",
    "    def buildModel(self):\n",
    "        \"\"\"Model layers\"\"\"\n",
    "        \n",
    "        # word-level input\n",
    "        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
    "                          trainable=False)(words_input)\n",
    "        \n",
    "        # case-info input\n",
    "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
    "                           trainable=False)(casing_input)\n",
    "\n",
    "        # character input\n",
    "        character_input = Input(shape=(None, 52,), name=\"Character_input\")\n",
    "        embed_char_out = TimeDistributed(\n",
    "            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
    "            character_input)\n",
    "\n",
    "        dropout = Dropout(self.dropout)(embed_char_out)\n",
    "\n",
    "        # CNN\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
    "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
    "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
    "        char = Dropout(self.dropout)(char)\n",
    "\n",
    "        # concat & BiLSTM\n",
    "        output = concatenate([words, casing, char])\n",
    "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
    "                                    return_sequences=True, \n",
    "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
    "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
    "                                   ), name=\"BiLSTM\")(output)\n",
    "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
    "\n",
    "        # set up model\n",
    "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        self.init_weights = self.model.get_weights()\n",
    "        \n",
    "        plot_model(self.model, to_file='model.png')\n",
    "        \n",
    "        print(\"Model built. Saved model.png\\n\")\n",
    "        \n",
    "    def compute_precision(guessed_sentences, correct_sentences):\n",
    "        assert(len(guessed_sentences) == len(correct_sentences))\n",
    "        correctCount = 0\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for sentenceIdx in range(len(guessed_sentences)):\n",
    "            guessed = guessed_sentences[sentenceIdx]\n",
    "            correct = correct_sentences[sentenceIdx]\n",
    "            assert(len(guessed) == len(correct))\n",
    "            idx = 0\n",
    "            while idx < len(guessed):\n",
    "                if guessed[idx][0] == 'B': #A new chunk starts\n",
    "                    count += 1\n",
    "\n",
    "                    if guessed[idx] == correct[idx]:\n",
    "                        idx += 1\n",
    "                        correctlyFound = True\n",
    "\n",
    "                        while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
    "                            if guessed[idx] != correct[idx]:\n",
    "                                correctlyFound = False\n",
    "\n",
    "                            idx += 1\n",
    "\n",
    "                        if idx < len(guessed):\n",
    "                            if correct[idx][0] == 'I': #The chunk in correct was longer\n",
    "                                correctlyFound = False\n",
    "\n",
    "\n",
    "                        if correctlyFound:\n",
    "                            correctCount += 1\n",
    "                    else:\n",
    "                        idx += 1\n",
    "                else:  \n",
    "                    idx += 1\n",
    "\n",
    "        precision = 0\n",
    "        if count > 0:    \n",
    "            precision = float(correctCount) / count\n",
    "\n",
    "        return precision\n",
    "    \n",
    "    def compute_f1(predictions, correct, idx2Label): \n",
    "        label_pred = []    \n",
    "        for sentence in predictions:\n",
    "            label_pred.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "        label_correct = []    \n",
    "        for sentence in correct:\n",
    "            label_correct.append([idx2Label[element] for element in sentence])\n",
    "\n",
    "\n",
    "        #print label_pred\n",
    "        #print label_correct\n",
    "\n",
    "        prec = compute_precision(label_pred, label_correct)\n",
    "        rec = compute_precision(label_correct, label_pred)\n",
    "\n",
    "        f1 = 0\n",
    "        if (rec+prec) > 0:\n",
    "            f1 = 2.0 * prec * rec / (prec + rec);\n",
    "\n",
    "        return prec, rec, f1\n",
    "    \n",
    "    def tag_dataset(self, dataset, model):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        b = Progbar(len(dataset))\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            correctLabels.append(labels)\n",
    "            predLabels.append(pred)\n",
    "            b.update(i)\n",
    "        b.update(i+1)\n",
    "        return predLabels, correctLabels\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Default training\"\"\"\n",
    "\n",
    "        self.f1_test_history = []\n",
    "        self.f1_dev_history = []\n",
    "\n",
    "        for epoch in range(self.epochs):    \n",
    "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
    "            a = Progbar(len(self.train_batch_len))\n",
    "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
    "                labels, tokens, casing, char = batch       \n",
    "                self.model.train_on_batch([tokens, casing,char], labels)\n",
    "                a.update(i)\n",
    "            a.update(i+1)\n",
    "            print(' ')    \n",
    "            \n",
    "    def evaluate(self):\n",
    "        #   Performance on dev dataset        \n",
    "        predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)        \n",
    "        pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
    "        print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "\n",
    "        #   Performance on test dataset       \n",
    "        predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)        \n",
    "        pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, self.idx2Label)\n",
    "        print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02d1200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set parameters\"\"\"\n",
    "\n",
    "EPOCHS =                 # paper: 80\n",
    "DROPOUT = 0.5             # paper: 0.68\n",
    "DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
    "LSTM_STATE_SIZE = 200     # paper: 275\n",
    "CONV_SIZE = 3             # paper: 3\n",
    "LEARNING_RATE = 0.0105    # paper 0.0105\n",
    "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629f8e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built. Saved model.png\n",
      "\n",
      "Epoch 0/3\n",
      "64/64 [==============================] - 50s 691ms/step\n",
      " \n",
      "Epoch 1/3\n",
      "64/64 [==============================] - 44s 694ms/step\n",
      " \n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 45s 718ms/step\n",
      " \n",
      "3250/3250 [==============================] - 283s 87ms/step\n",
      "Dev-Data: Prec: 0.708, Rec: 0.653, F1: 0.680\n",
      "3453/3453 [==============================] - 298s 86ms/step\n",
      "Test-Data: Prec: 0.639, Rec: 0.638, F1: 0.638\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Construct and run model\"\"\"\n",
    "\n",
    "bilstm_cnn = BiLSTM_CNN(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "bilstm_cnn.loadData()\n",
    "bilstm_cnn.addCharInfo()\n",
    "bilstm_cnn.embed()\n",
    "bilstm_cnn.createBatches()\n",
    "bilstm_cnn.buildModel()\n",
    "bilstm_cnn.train()\n",
    "bilstm_cnn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821d5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
